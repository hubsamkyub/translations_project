import os
import time
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
import pandas as pd
import sqlite3
import tkinter as tk
import signal
import win32com.client as pythoncom
import xlwings as xw
from collections import defaultdict
import hashlib

class EnhancedTranslationApplyManager:
    def __init__(self, parent_window=None):
        self.parent_ui = parent_window
        
        # ê¸°ë³¸ ìºì‹œ
        self.translation_cache = {}
        self.translation_file_cache = {}
        self.translation_sheet_cache = {}
        self.duplicate_ids = {} # ì´ ë¶€ë¶„ì€ DB ê¸°ë°˜ìœ¼ë¡œ ë³€ê²½ë˜ë©´ì„œ ì‚¬ìš©ì„±ì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆìŒ
        self.kr_reverse_cache = {}
        
        # [ì‹ ê·œ] íŠ¹ìˆ˜ ì»¬ëŸ¼ ê´€ë ¨ ìºì‹œ
        self.special_filtered_cache = {}  # íŠ¹ìˆ˜ ì»¬ëŸ¼ í•„í„°ë§ëœ ë°ì´í„°
        self.detected_special_columns = {}  # ê°ì§€ëœ íŠ¹ìˆ˜ ì»¬ëŸ¼ ì •ë³´
        self.current_filter_conditions = {}  # í˜„ì¬ ì ìš©ëœ í•„í„° ì¡°ê±´
        
        # ì§€ì› ì–¸ì–´ ì œí•œ
        self.supported_languages = ["KR", "CN", "TW"]
        
        # [ì‹ ê·œ] DB ìºì‹œ í´ë”
        self.cache_dir = os.path.join(os.getcwd(), "temp_db")
        os.makedirs(self.cache_dir, exist_ok=True)
            
        self.SPECIAL_COLUMN_NAMES = [
            "ë²ˆì—­ìš”ì²­", 
            "ìˆ˜ì •ìš”ì²­",
            "Help",
            "ì›ë¬¸",
            "ë²ˆì—­ìš”ì²­2",
            "ë²ˆì—­ì¶”ê°€",
            "ë²ˆì—­ì ìš©"
            # í•„ìš”í•œ ë‹¤ë¥¸ íŠ¹ìˆ˜ ì»¬ëŸ¼ ì´ë¦„ì„ ì—¬ê¸°ì— ì¶”ê°€
        ]

    ### safe_strip: [ë³€ê²½ ì—†ìŒ]
    def safe_strip(self, value):
        if value is None:
            return ""
        return str(value).strip()

    ### safe_lower: [ë³€ê²½ ì—†ìŒ]
    def safe_lower(self, value):
        if value is None:
            return ""
        return str(value).lower().strip()

    ### log_message: [ë³€ê²½ ì—†ìŒ]
    def log_message(self, message):
        if self.parent_ui and hasattr(self.parent_ui, 'log_text'):
            self.parent_ui.log_text.insert(tk.END, f"{message}\n")
            self.parent_ui.log_text.see(tk.END)
            self.parent_ui.update_idletasks()
        else:
            print(message)

    ### _get_db_path: [ì‹ ê·œ]
    def _get_db_path(self, excel_path):
        """[ì‹ ê·œ] ì—‘ì…€ íŒŒì¼ ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ í•œ DB ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # íŒŒì¼ ê²½ë¡œë¥¼ í•´ì‹±í•˜ì—¬ ê³ ìœ í•˜ê³  ì•ˆì „í•œ íŒŒì¼ ì´ë¦„ì„ ë§Œë“­ë‹ˆë‹¤.
        file_hash = hashlib.md5(excel_path.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"cache_{file_hash}.db")

    ### _is_db_cache_valid: [ì‹ ê·œ]
    def _is_db_cache_valid(self, excel_path, db_path):
        """[ì‹ ê·œ] DB ìºì‹œê°€ ìµœì‹  ìƒíƒœì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        if not os.path.exists(db_path):
            return False
            
        try:
            # ì›ë³¸ ì—‘ì…€ íŒŒì¼ì˜ ìµœì¢… ìˆ˜ì • ì‹œê°„
            excel_mod_time = os.path.getmtime(excel_path)
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT value FROM metadata WHERE key = 'source_mod_time'")
            stored_mod_time = cursor.fetchone()
            conn.close()
            
            if stored_mod_time and float(stored_mod_time[0]) == excel_mod_time:
                self.log_message("âœ… ìœ íš¨í•œ DB ìºì‹œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ìºì‹œë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")
                return True
            else:
                self.log_message("âš ï¸ ì›ë³¸ íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. DB ìºì‹œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
                return False
        except Exception as e:
            self.log_message(f"DB ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}. ìºì‹œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
            return False

    ### _build_db_from_excel: [ì‹ ê·œ]
    def _build_db_from_excel(self, excel_path, db_path, progress_callback=None):
        """[ì‹ ê·œ] ì—‘ì…€ íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ì½ì–´ SQLite DB ìºì‹œë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        try:
            self.log_message(f"âš™ï¸ DB ìºì‹œ êµ¬ì¶• ì‹œì‘: {os.path.basename(excel_path)}")
            
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            cursor.execute('DROP TABLE IF EXISTS metadata')
            cursor.execute('DROP TABLE IF EXISTS translation_data')
            conn.commit()

            cursor.execute('''
                CREATE TABLE metadata (key TEXT PRIMARY KEY, value TEXT)
            ''')
            cursor.execute('''
                CREATE TABLE translation_data (
                    string_id TEXT PRIMARY KEY, kr TEXT, cn TEXT, tw TEXT,
                    file_name TEXT, sheet_name TEXT, special_columns TEXT 
                )
            ''')
            
            wb = load_workbook(excel_path, read_only=True, data_only=True)
            all_sheets = wb.sheetnames

            for idx, sheet_name in enumerate(all_sheets):
                if progress_callback:
                    progress_callback((idx / len(all_sheets)) * 100, f"ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘ ({idx+1}/{len(all_sheets)}): {sheet_name}")

                if not sheet_name.lower().startswith("string") or sheet_name.startswith("#"):
                    continue
                
                ws = wb[sheet_name]

                header_map = {}
                header_row_idx = -1
                for i, row in enumerate(ws.iter_rows(min_row=1, max_row=10, values_only=True)):
                    cleaned_row = [self.safe_lower(str(cell)) for cell in row if cell is not None]
                    if 'string_id' in cleaned_row:
                        header_row_idx = i + 1
                        for col_idx, header_val in enumerate(row, 1):
                            if header_val:
                                header_map[self.safe_strip(str(header_val))] = col_idx
                        break
                
                if 'string_id' not in [self.safe_lower(k) for k in header_map.keys()]:
                    continue

                string_id_col_key = [k for k in header_map if self.safe_lower(k) == 'string_id'][0]
                string_id_col = header_map[string_id_col_key]

                rows_to_insert = []
                for row_data in ws.iter_rows(min_row=header_row_idx + 1, values_only=True):
                    if not row_data or len(row_data) < string_id_col or not row_data[string_id_col-1]:
                        continue
                    
                    string_id = self.safe_strip(str(row_data[string_id_col-1]))
                    if not string_id:
                        continue
                        
                    data = {}
                    special_data = {}
                    # â–¼â–¼â–¼ í—¤ë” ì •ê·œí™” ë¡œì§ ì ìš© â–¼â–¼â–¼
                    for header, col in header_map.items():
                        value = self.safe_strip(row_data[col-1]) if len(row_data) >= col else ""

                        # í—¤ë” ì´ë¦„ì—ì„œ # ì œê±°, ëª¨ë“  ê³µë°± ì œê±°í•˜ì—¬ ì •ê·œí™”
                        normalized_header = header.lstrip('#').replace(' ', '')

                        # ì •ê·œí™”ëœ ì´ë¦„ì´ SPECIAL_COLUMN_NAMES ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
                        if normalized_header in self.SPECIAL_COLUMN_NAMES:
                            # DBì— ì €ì¥í•  ë•Œë„ ì •ê·œí™”ëœ ì´ë¦„(ì˜ˆ: 'ìˆ˜ì •ìš”ì²­')ì„ í‚¤ë¡œ ì‚¬ìš©
                            special_data[normalized_header] = value
                        else:
                            # ì¼ë°˜ ì»¬ëŸ¼ì€ ì†Œë¬¸ìë¡œ í†µì¼
                            data[self.safe_lower(header)] = value

                    # special_columnsë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    import json
                    special_columns_json = json.dumps(special_data, ensure_ascii=False)

                    kr_val = data.get("kr", "")
                    cn_val = data.get("cn", "")
                    tw_val = data.get("tw", "")
                    file_val = data.get("filename", data.get("file_name", os.path.basename(excel_path)))
                    sheet_val = data.get("sheetname", data.get("sheet_name", sheet_name))

                    rows_to_insert.append((
                        string_id, kr_val, cn_val, tw_val, file_val, sheet_val, special_columns_json
                    ))

                cursor.executemany('''
                    INSERT OR REPLACE INTO translation_data 
                    (string_id, kr, cn, tw, file_name, sheet_name, special_columns)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', rows_to_insert)

            excel_mod_time = os.path.getmtime(excel_path)
            cursor.execute("INSERT INTO metadata (key, value) VALUES (?, ?)", ('source_mod_time', str(excel_mod_time)))
            
            conn.commit()
            conn.close()
            self.log_message("âœ… DB ìºì‹œ êµ¬ì¶• ì™„ë£Œ.")
            return {"status": "success"}
        except Exception as e:
            self.log_message(f"âŒ DB ìºì‹œ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    ### initiate_excel_caching: [ì‹ ê·œ]
    def initiate_excel_caching(self, excel_path, force_rebuild=False, progress_callback=None):
        """[ìˆ˜ì •] force_rebuild í”Œë˜ê·¸ë¥¼ ì¶”ê°€í•˜ì—¬ DB ìºì‹œ ê°•ì œ ì¬êµ¬ì¶•ì„ ì§€ì›í•©ë‹ˆë‹¤."""
        db_path = self._get_db_path(excel_path)

        # â–¼â–¼â–¼ force_rebuild ë¡œì§ ì¶”ê°€ â–¼â–¼â–¼
        if force_rebuild:
            self.log_message("â„¹ï¸ ì‚¬ìš©ìì˜ ìš”ì²­ìœ¼ë¡œ DB ìºì‹œë¥¼ ê°•ì œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
        elif self._is_db_cache_valid(excel_path, db_path):
            # ìºì‹œê°€ ìœ íš¨í•˜ë©´ ì‹œíŠ¸ ëª©ë¡ë§Œ DBì—ì„œ ê°€ì ¸ì™€ì„œ ë°˜í™˜ (ê°•ì œ ì¬êµ¬ì¶•ì´ ì•„ë‹ ë•Œë§Œ)
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT DISTINCT sheet_name FROM translation_data ORDER BY sheet_name")
                sheets = [row[0] for row in cursor.fetchall()]
                conn.close()
                return {"status": "success", "source_type": "DB_Cache", "sheets": sheets}
            except Exception as e:
                self.log_message(f"ìºì‹œëœ ì‹œíŠ¸ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")

        # ìºì‹œê°€ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ê°•ì œ ì¬êµ¬ì¶•ì´ ìš”ì²­ëœ ê²½ìš° ìƒˆë¡œ êµ¬ì¶•
        build_result = self._build_db_from_excel(excel_path, db_path, progress_callback)
        if build_result["status"] == "success":
            # êµ¬ì¶• ì„±ê³µ í›„, ì¬ê·€ í˜¸ì¶œ ëŒ€ì‹  ì§ì ‘ ì‹œíŠ¸ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ ë°˜í™˜í•˜ë„ë¡ ìµœì í™”
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT DISTINCT sheet_name FROM translation_data ORDER BY sheet_name")
                sheets = [row[0] for row in cursor.fetchall()]
                conn.close()
                return {"status": "success", "source_type": "DB_Cache", "sheets": sheets}
            except Exception as e:
                self.log_message(f"ì¬êµ¬ì¶• í›„ ì‹œíŠ¸ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
                return {"status": "error", "message": "DB ì¬êµ¬ì¶• í›„ ì‹œíŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        else:
            return build_result
        
    ### load_translation_cache_from_excel_with_filter: [ë³€ê²½]
    def load_translation_cache_from_excel_with_filter(self, excel_path, sheet_names, special_column_filter=None):
        """[ë³€ê²½] DB ìºì‹œì—ì„œ ë°ì´í„°ë¥¼ ì¿¼ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ìºì‹œë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
        self.log_message("âš™ï¸ DB ìºì‹œë¡œë¶€í„° ë©”ëª¨ë¦¬ ìºì‹œ ë¡œë”© ì‹œì‘...")
        db_path = self._get_db_path(excel_path)
        if not os.path.exists(db_path):
            return {"status": "error", "message": "DB ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ìºì‹œë¥¼ ìƒì„±í•˜ì„¸ìš”."}

        try:
            # ìºì‹œ ì´ˆê¸°í™”
            self.translation_cache = {}
            self.kr_reverse_cache = {}
            self.special_filtered_cache = {}
            # file/sheet cacheëŠ” DBê¸°ë°˜ì—ì„œëŠ” ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ê´€ë¦¬ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì´ˆê¸°í™”ë§Œ í•¨
            self.translation_file_cache = {} 
            self.translation_sheet_cache = {}
            
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # ê¸°ë³¸ ì¿¼ë¦¬ë¬¸
            query = "SELECT * FROM translation_data"
            params = []
            
            # ì‹œíŠ¸ ì´ë¦„ ì¡°ê±´ ì¶”ê°€
            if sheet_names:
                query += f" WHERE sheet_name IN ({','.join('?' for _ in sheet_names)})"
                params.extend(sheet_names)

            cursor.execute(query, params)
            rows = cursor.fetchall()

            total_loaded = 0
            total_filtered = 0

            for row in rows:
                data = dict(row)
                string_id = data["string_id"]
                
                # ë©”ëª¨ë¦¬ ìºì‹œ êµ¬ì„±
                self.translation_cache[string_id] = data
                kr_text = data.get("kr", "")
                if kr_text and kr_text not in self.kr_reverse_cache:
                    self.kr_reverse_cache[kr_text] = {**data}
                
                total_loaded += 1

                # íŠ¹ìˆ˜ ì»¬ëŸ¼ í•„í„°ë§ ë¡œì§
                if special_column_filter:
                    filter_column = special_column_filter['column_name']
                    filter_value = special_column_filter['condition_value']
                    
                    import json
                    special_columns = json.loads(data.get('special_columns', '{}'))
                    
                    if filter_column in special_columns:
                        special_cell_value = special_columns[filter_column]
                        if self.safe_lower(filter_value) in self.safe_lower(special_cell_value):
                            self.special_filtered_cache[string_id] = {**data, **special_columns}
                            total_filtered += 1
            
            conn.close()

            # ê²°ê³¼ ë¡œê·¸
            self.log_message(f"ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œ êµ¬ì„± ì™„ë£Œ:")
            self.log_message(f" Â - ì„ íƒëœ ì‹œíŠ¸ë¡œë¶€í„° ë¡œë“œ: {total_loaded}ê°œ")
            if special_column_filter:
                self.log_message(f" Â - í•„í„°ë§ëœ ë°ì´í„°: {total_filtered}ê°œ ({filter_column} = '{special_column_filter['condition_value']}')")

            return {
                "status": "success", "source_type": "Excel_DB_Cache",
                "id_count": len(self.translation_cache),
                "filtered_count": len(self.special_filtered_cache),
                "translation_cache": self.translation_cache,
                "special_filtered_cache": self.special_filtered_cache,
                "kr_reverse_cache": self.kr_reverse_cache,
                "duplicate_ids": {} # ì´ ë¡œì§ì€ ë‹¨ìˆœí™”ë¨
            }
        except Exception as e:
            self.log_message(f"âŒ DB ìºì‹œ ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    ### load_translation_cache_from_excel: [ë³€ê²½]
    def load_translation_cache_from_excel(self, file_path, sheet_names):
        """[í˜¸í™˜ì„±] ê¸°ì¡´ ë©”ì„œë“œëŠ” í•„í„°ë§ ì—†ëŠ” ìƒˆ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        return self.load_translation_cache_from_excel_with_filter(file_path, sheet_names, None)

    def apply_translation_with_filter_option(self, file_path, options):
        """[ìˆ˜ì •] ì¤‘ë³µ ë¡œê·¸ ì œê±° ë° ë®ì–´ì“°ê¸°ëœ í•­ëª© ìƒì„¸ ì •ë³´ ë°˜í™˜ ê¸°ëŠ¥ ì¶”ê°€"""

        mode = options.get("mode", "id")
        selected_langs = options.get("selected_langs", [])
        record_date = options.get("record_date", True)
        use_filtered_data = options.get("use_filtered_data", False)

        kr_match_check = options.get("kr_match_check", True)
        kr_mismatch_delete = options.get("kr_mismatch_delete", False)
        kr_overwrite = options.get("kr_overwrite", False)

        kr_overwrite_on_kr_mode = options.get("kr_overwrite_on_kr_mode", False)

        allowed_statuses = options.get("allowed_statuses", [])
        allowed_statuses_lower = [status.lower() for status in allowed_statuses] if allowed_statuses else []

        if use_filtered_data and self.special_filtered_cache:
            active_cache = self.special_filtered_cache
            cache_type = "íŠ¹ìˆ˜í•„í„°ë§"
            # â–¼â–¼â–¼ [ìš”ì²­ 1] ì´ ìœ„ì¹˜ì˜ ë¡œê·¸ ë©”ì‹œì§€ ì œê±° â–¼â–¼â–¼
            # self.log_message(f"ğŸ” íŠ¹ìˆ˜ í•„í„°ë§ëœ ìºì‹œ ì‚¬ìš©: {len(active_cache)}ê°œ í•­ëª©")
        else:
            active_cache = self.translation_cache
            cache_type = "ì „ì²´"

        if not active_cache:
            return {"status": "error", "message": f"{cache_type} ë²ˆì—­ ìºì‹œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        if mode == 'kr' and not self.kr_reverse_cache:
            return {"status": "error", "message": "KR ê¸°ë°˜ ì ìš©ì„ ìœ„í•œ ì—­ë°©í–¥ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤."}

        file_name = os.path.basename(file_path)

        option_summary = []
        option_summary.append(f"{mode.upper()} ê¸°ë°˜")
        option_summary.append(f"{cache_type} ìºì‹œ")

        if mode == 'id' and kr_match_check:
            option_summary.append("KRì¼ì¹˜ê²€ì‚¬")
            if kr_mismatch_delete:
                option_summary.append("ë¶ˆì¼ì¹˜ì‹œì‚­ì œ")
            if kr_overwrite:
                option_summary.append("ë®ì–´ì“°ê¸°")
        elif mode == 'kr' and kr_overwrite_on_kr_mode:
            option_summary.append("ë®ì–´ì“°ê¸°")

        if allowed_statuses:
            option_summary.append(f"ì¡°ê±´:{','.join(allowed_statuses)}")

        self.log_message(f"ğŸ“ {file_name} ì²˜ë¦¬ì‹œì‘ [{' | '.join(option_summary)}]")

        workbook = None
        try:
            workbook = load_workbook(file_path)

            string_sheets = [sheet for sheet in workbook.sheetnames if sheet.lower().startswith("string") and not sheet.startswith("#")]

            if not string_sheets:
                self.log_message(f" Â  âš ï¸ String ì‹œíŠ¸ ì—†ìŒ")
                return {"status": "info", "message": "íŒŒì¼ì— String ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}

            file_modified = False
            results = {
                "total_updated": 0, "total_overwritten": 0, "total_kr_mismatch_skipped": 0,
                "total_kr_mismatch_deleted": 0, "total_conditional_skipped": 0
            }

            # â–¼â–¼â–¼ [ìš”ì²­ 3] ë®ì–´ì“´ í•­ëª©ì„ ê¸°ë¡í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™” â–¼â–¼â–¼
            overwritten_items = []

            sheet_details = {}

            fill_green = PatternFill(start_color="DAF2D0", end_color="DAF2D0", fill_type="solid")
            fill_orange = PatternFill(start_color="FFDDC1", end_color="FFDDC1", fill_type="solid")
            fill_blue = PatternFill(start_color="D0E7FF", end_color="D0E7FF", fill_type="solid")

            for sheet_name in string_sheets:
                worksheet = workbook[sheet_name]
                string_id_col, header_row = self.find_string_id_position(worksheet)
                if not string_id_col or not header_row:
                    self.log_message(f" Â  âš ï¸ {sheet_name}: STRING_ID ì»¬ëŸ¼ ì—†ìŒ")
                    continue

                supported_langs = [lang for lang in selected_langs if lang in self.supported_languages]
                lang_cols = self.find_language_columns(worksheet, header_row, supported_langs + ['KR'])
                request_col_idx = self.find_target_columns(worksheet, header_row, ["#ë²ˆì—­ìš”ì²­"]).get("#ë²ˆì—­ìš”ì²­")

                sheet_stats = {
                    "updated": 0, "overwritten": 0, "conditional_skipped": 0,
                    "kr_mismatch_skipped": 0, "kr_mismatch_deleted": 0,
                    "total_rows": 0, "processed_rows": 0
                }

                lang_apply_count = {lang: 0 for lang in supported_langs if lang != 'KR'}
                sheet_stats["total_rows"] = worksheet.max_row - header_row

                for row_idx in range(header_row + 1, worksheet.max_row + 1):
                    sheet_stats["processed_rows"] += 1

                    if allowed_statuses_lower and request_col_idx:
                        request_val = self.safe_lower(str(worksheet.cell(row=row_idx, column=request_col_idx).value or ''))
                        if request_val not in allowed_statuses_lower:
                            sheet_stats["conditional_skipped"] += 1
                            continue

                    trans_data = None
                    key_value = ''
                    if mode == 'id':
                        key_value = self.safe_strip(str(worksheet.cell(row=row_idx, column=string_id_col).value or ''))
                        if key_value:
                            trans_data = active_cache.get(key_value)
                    else: 
                        if 'KR' in lang_cols:
                            key_value = self.safe_strip(str(worksheet.cell(row=row_idx, column=lang_cols['KR']).value or ''))
                            if key_value:
                                trans_data = self.kr_reverse_cache.get(key_value)

                    if not key_value or not trans_data:
                        continue

                    row_modified_this_iteration = False

                    if mode == 'id' and kr_match_check:
                        current_kr_val = self.safe_strip(str(worksheet.cell(row=row_idx, column=lang_cols['KR']).value or ''))
                        cache_kr_val = self.safe_strip(str(trans_data.get('kr', '')))
                        if current_kr_val != cache_kr_val:
                            if kr_mismatch_delete:
                                deleted_count = 0
                                for lang, col_idx in lang_cols.items():
                                    if lang != 'KR' and worksheet.cell(row=row_idx, column=col_idx).value:
                                        worksheet.cell(row=row_idx, column=col_idx).value = ""
                                        deleted_count += 1
                                        row_modified_this_iteration = True
                                if deleted_count > 0:
                                    sheet_stats["kr_mismatch_deleted"] += 1
                            else:
                                sheet_stats["kr_mismatch_skipped"] += 1
                            continue 

                    for lang in supported_langs:
                        if lang == 'KR': continue

                        lang_lower = lang.lower()
                        col_idx = lang_cols.get(lang)
                        if not col_idx: continue

                        cell = worksheet.cell(row=row_idx, column=col_idx)
                        current_val = self.safe_strip(str(cell.value or ''))
                        cached_val = self.safe_strip(str(trans_data.get(lang_lower, '')))

                        if cached_val and current_val != cached_val:
                            should_overwrite = (mode == 'id' and kr_match_check and kr_overwrite) or \
                                            (mode == 'kr' and kr_overwrite_on_kr_mode)

                            if should_overwrite:
                                cell.value = cached_val
                                cell.fill = fill_orange
                                sheet_stats["overwritten"] += 1
                                lang_apply_count[lang] += 1
                                row_modified_this_iteration = True
                                # â–¼â–¼â–¼ [ìš”ì²­ 3] ë®ì–´ì“°ê¸° ì •ë³´ ìˆ˜ì§‘ â–¼â–¼â–¼
                                overwritten_items.append({
                                    "file_name": file_name,
                                    "sheet_name": sheet_name,
                                    "string_id": key_value,
                                    "language": lang,
                                    "kr_text": trans_data.get('kr', ''),
                                    "overwritten_text": cached_val
                                })
                                # â–²â–²â–² ì—¬ê¸°ê¹Œì§€ ì¶”ê°€ â–²â–²â–²
                            elif not current_val: # ë¹ˆ ì¹¸ì—ë§Œ ì ìš©
                                cell.value = cached_val
                                cell.fill = fill_blue if use_filtered_data else fill_green
                                sheet_stats["updated"] += 1
                                lang_apply_count[lang] += 1
                                row_modified_this_iteration = True

                    if row_modified_this_iteration:
                        file_modified = True
                        if record_date and request_col_idx:
                            worksheet.cell(row=row_idx, column=request_col_idx).value = "íŠ¹ìˆ˜í•„í„°ì ìš©" if use_filtered_data else "ì ìš©"

                if sheet_stats["updated"] > 0 or sheet_stats["overwritten"] > 0:
                    lang_details = [f"{lang}:{count}" for lang, count in lang_apply_count.items() if count > 0]
                    log_parts = []
                    if sheet_stats["updated"] > 0: log_parts.append(f"ì‹ ê·œ:{sheet_stats['updated']}")
                    if sheet_stats["overwritten"] > 0: log_parts.append(f"ë®ì–´ì”€:{sheet_stats['overwritten']}")
                    if lang_details: log_parts.append(f"[{', '.join(lang_details)}]")
                    self.log_message(f" Â  âœ… {sheet_name}: {' | '.join(log_parts)}")
                else:
                    skip_reasons = []
                    if sheet_stats["conditional_skipped"] > 0: skip_reasons.append(f"ì¡°ê±´ë¶ˆì¼ì¹˜:{sheet_stats['conditional_skipped']}")
                    if sheet_stats["kr_mismatch_skipped"] > 0: skip_reasons.append(f"KRë¶ˆì¼ì¹˜:{sheet_stats['kr_mismatch_skipped']}")
                    if sheet_stats["kr_mismatch_deleted"] > 0: skip_reasons.append(f"KRë¶ˆì¼ì¹˜ì‚­ì œ:{sheet_stats['kr_mismatch_deleted']}")

                    if skip_reasons: self.log_message(f" Â  âš ï¸ {sheet_name}: ì ìš©ì—†ìŒ ({' | '.join(skip_reasons)})")
                    else: self.log_message(f" Â  âš ï¸ {sheet_name}: ì ìš©ì—†ìŒ (ë²ˆì—­ë°ì´í„° ì—†ìŒ)")

                for key in results:
                    if key.startswith("total_"):
                        results[key] += sheet_stats.get(key[6:], 0)

                sheet_details[sheet_name] = sheet_stats

            if file_modified:
                self.log_message(f" Â  ğŸ’¾ ë³€ê²½ì‚¬í•­ ì €ì¥ ì¤‘...")
                workbook.save(file_path)

                summary_parts = []
                if results["total_updated"] > 0: summary_parts.append(f"ì‹ ê·œ {results['total_updated']}ê°œ")
                if results["total_overwritten"] > 0: summary_parts.append(f"ë®ì–´ì”€ {results['total_overwritten']}ê°œ")

                total_applied = results["total_updated"] + results["total_overwritten"]
                cache_info = f"({cache_type}ìºì‹œ)" if use_filtered_data else ""
                self.log_message(f" Â  âœ… {file_name} ì™„ë£Œ: {' | '.join(summary_parts)} (ì´ {total_applied}ê°œ ì ìš©) {cache_info}")
            else:
                skip_summary = []
                if results["total_conditional_skipped"] > 0: skip_summary.append(f"ì¡°ê±´ {results['total_conditional_skipped']}ê°œ")
                if results["total_kr_mismatch_skipped"] > 0: skip_summary.append(f"KRë¶ˆì¼ì¹˜ {results['total_kr_mismatch_skipped']}ê°œ")

                if skip_summary: self.log_message(f" Â  âš ï¸ {file_name} ì™„ë£Œ: ë³€ê²½ì—†ìŒ ({' | '.join(skip_summary)} ê±´ë„ˆëœ€)")
                else: self.log_message(f" Â  âš ï¸ {file_name} ì™„ë£Œ: ë³€ê²½ì—†ìŒ (ë²ˆì—­ ë°ì´í„° ì—†ìŒ)")

            # â–¼â–¼â–¼ [ìš”ì²­ 3] ìˆ˜ì§‘í•œ ë®ì–´ì“°ê¸° ëª©ë¡ì„ ë°˜í™˜ê°’ì— ì¶”ê°€ â–¼â–¼â–¼
            return {"status": "success", **results, "overwritten_items": overwritten_items}

        except Exception as e:
            self.log_message(f" Â  âŒ {file_name} ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e), "error_type": "processing_error"}
        finally:
            if workbook:
                workbook.close()
 
    ### apply_translation: [ë³€ê²½ ì—†ìŒ]
    def apply_translation(self, file_path, options):
        return self.apply_translation_with_filter_option(file_path, options)

    ### find_string_id_position: [ë³€ê²½ ì—†ìŒ]
    def find_string_id_position(self, worksheet):
        for row in range(2, 6):
            for col in range(1, min(10, worksheet.max_column + 1)):
                cell_value = worksheet.cell(row=row, column=col).value
                if cell_value and isinstance(cell_value, str):
                    if "string_id" in self.safe_lower(cell_value):
                        return col, row
                        
        for row in worksheet.iter_rows(min_row=1, max_row=1, max_col=5):
            for cell in row:
                if cell.value and isinstance(cell.value, str):
                    if "string_id" in self.safe_lower(cell.value):
                        return cell.column, cell.row
                        
        return None, None

    ### find_language_columns: [ë³€ê²½ ì—†ìŒ]
    def find_language_columns(self, worksheet, header_row, langs):
        if not header_row:
            return {}
            
        lang_cols = {}
        
        langs_upper = [lang.upper() for lang in langs]
        
        for row in worksheet.iter_rows(min_row=header_row, max_row=header_row):
            for cell in row:
                if not cell.value:
                    continue
                    
                header_text = self.safe_strip(cell.value).upper()
                
                if header_text in langs_upper:
                    lang_cols[header_text] = cell.column
                    
        return lang_cols

    ### find_target_columns: [ë³€ê²½ ì—†ìŒ]
    def find_target_columns(self, worksheet, header_row, target_columns=None):
        if not header_row:
            return {}
            
        found_columns = {}
        all_targets = ["#ë²ˆì—­ìš”ì²­"]
        if target_columns:
            all_targets.extend(target_columns)
        
        all_targets = list(set(all_targets))

        for cell in worksheet[header_row]:
            if cell.value and isinstance(cell.value, str):
                cell_value_clean = self.safe_strip(cell.value).lower()
                for target in all_targets:
                    if cell_value_clean == target.lower():
                        found_columns[target] = cell.column
                        break
                        
        return found_columns

    ### get_filter_statistics: [ì‚­ì œ]
    # ì´ í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ DB ê¸°ë°˜ ë¡œì§ê³¼ ë§ì§€ ì•Šì•„ ì‚­ì œí•˜ê³ , í•„ìš”ì‹œ DB ì¿¼ë¦¬ë¥¼ í†µí•´ ì§ì ‘ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

    ### load_translation_cache_from_db: [ë³€ê²½ ì—†ìŒ]
    # ì´ í•¨ìˆ˜ëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ ì œê³µí•˜ëŠ” ë³„ë„ì˜ DB íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ, ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    def load_translation_cache_from_db(self, db_path):
        """[ê¸°ì¡´] ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ì ‘ ë²ˆì—­ ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            self.log_message(f"âš™ï¸ DB ë¡œë”© ì‹œì‘: {db_path}")

            # ìºì‹œ ì´ˆê¸°í™”
            self.translation_cache = {}
            self.translation_file_cache = {}
            self.translation_sheet_cache = {}
            self.duplicate_ids = {}
            self.kr_reverse_cache = {}

            conn = sqlite3.connect(db_path)
            # ì»¬ëŸ¼ ì´ë¦„ìœ¼ë¡œ ë°ì´í„°ì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ row_factory ì„¤ì •
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='translation_data'")
            if cursor.fetchone() is None:
                conn.close()
                message = "'translation_data' í…Œì´ë¸”ì´ DBì— ì—†ìŠµë‹ˆë‹¤."
                self.log_message(f"âŒ {message}")
                return {"status": "error", "message": message}

            # 'active' ìƒíƒœì¸ ë°ì´í„°ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            cursor.execute("SELECT * FROM translation_data WHERE status = 'active'")
            rows = cursor.fetchall()
            conn.close()

            # DataFrameì„ ê±°ì¹˜ì§€ ì•Šê³  ì§ì ‘ ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            for row in rows:
                string_id = row["string_id"]
                if not string_id:
                    continue
                
                # sqlite3.Row ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
                data = dict(row)
                
                # [ê°œì„ ] ëª¨ë“  í…ìŠ¤íŠ¸ ê°’ì— TRIM ì ìš©
                cleaned_data = {}
                for key, value in data.items():
                    if isinstance(value, str):
                        cleaned_data[key] = self.safe_strip(value)
                    else:
                        cleaned_data[key] = value
                
                file_name_val = cleaned_data.get("file_name", "")
                sheet_name_val = cleaned_data.get("sheet_name", "")

                # ë‹¤ì¤‘ ìºì‹œ êµ¬ì„±
                if file_name_val:
                    self.translation_file_cache.setdefault(file_name_val.lower(), {})[string_id] = cleaned_data
                if sheet_name_val:
                    self.translation_sheet_cache.setdefault(sheet_name_val.lower(), {})[string_id] = cleaned_data
                self.translation_cache[string_id] = cleaned_data

                # KR ì—­ë°©í–¥ ì¡°íšŒ ìºì‹œ ìƒì„±
                kr_text = cleaned_data.get("kr", "")
                if kr_text:
                    kr_text = self.safe_strip(kr_text)
                    if kr_text and kr_text not in self.kr_reverse_cache:
                        self.kr_reverse_cache[kr_text] = {**cleaned_data}
            
            self.log_message(f"ğŸ”§ DB ìºì‹œ êµ¬ì„± ì™„ë£Œ (ID: {len(self.translation_cache)}, íŒŒì¼: {len(self.translation_file_cache)}, ì‹œíŠ¸: {len(self.translation_sheet_cache)})")

            return {
                "status": "success",
                "source_type": "DB",
                "id_count": len(self.translation_cache),
                "file_count": len(self.translation_file_cache),
                "sheet_count": len(self.translation_sheet_cache),
                "translation_cache": self.translation_cache,
                "translation_file_cache": self.translation_file_cache,
                "translation_sheet_cache": self.translation_sheet_cache,
                "duplicate_ids": {},
                "kr_reverse_cache": self.kr_reverse_cache
            }
        except Exception as e:
            self.log_message(f"âŒ ë²ˆì—­ DB ìºì‹œ ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    def detect_special_column_in_excel(self, excel_path, target_column_name):
        """[ë³€ê²½] DB ìºì‹œì—ì„œ íŠ¹ìˆ˜ ì»¬ëŸ¼ ì •ë³´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        db_path = self._get_db_path(excel_path)
        if not os.path.exists(db_path):
            return {"status": "error", "message": "DB ìºì‹œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}

        self.log_message(f"âš™ï¸ [íŠ¹ìˆ˜ì»¬ëŸ¼ê°ì§€] DB ìºì‹œì—ì„œ '{target_column_name}' ë¶„ì„ ì‹œì‘...")

        # â–¼â–¼â–¼ ì‚¬ìš©ì ì…ë ¥ê°’ ì •ê·œí™” â–¼â–¼â–¼
        normalized_target_column = target_column_name.lstrip('#').replace(' ', '')
        self.log_message(f"   (ì •ê·œí™”ëœ ê²€ìƒ‰ì–´: '{normalized_target_column}')")
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT special_columns, sheet_name FROM translation_data")
            rows = cursor.fetchall()
            conn.close()
            
            import json
            values_count = defaultdict(int)
            found_in_sheets = set()
            non_empty_rows = 0

            for special_json, sheet_name in rows:
                if not special_json: continue
                
                special_data = json.loads(special_json)
                if normalized_target_column in special_data:
                    value = self.safe_strip(special_data[normalized_target_column])
                    if value:
                        values_count[value] += 1
                        found_in_sheets.add(sheet_name)
                        non_empty_rows += 1
            
            if not values_count:
                self.log_message(f"âš ï¸ íŠ¹ìˆ˜ ì»¬ëŸ¼ '{target_column_name}'ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "success", "detected_info": {}}

            most_common = sorted(values_count.items(), key=lambda x: x[1], reverse=True)[:5]
            
            analysis_result = {
                'non_empty_rows': non_empty_rows,
                'unique_values': dict(values_count),
                'most_common': most_common,
                'found_in_sheets': list(found_in_sheets)
            }

            self.log_message(f"âœ… íŠ¹ìˆ˜ ì»¬ëŸ¼ '{target_column_name}' ë¶„ì„ ì™„ë£Œ:")
            self.log_message(f" Â - ë°œê²¬ëœ ì‹œíŠ¸: {len(found_in_sheets)}ê°œ")
            self.log_message(f" Â - ë°ì´í„° í•­ëª©: {non_empty_rows}ê°œ / ê³ ìœ ê°’: {len(values_count)}ê°œ")
            self.log_message(f" Â - ìµœë¹ˆê°’: {most_common}")

            return {"status": "success", "detected_info": analysis_result}

        except Exception as e:
            self.log_message(f"âŒ íŠ¹ìˆ˜ ì»¬ëŸ¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}