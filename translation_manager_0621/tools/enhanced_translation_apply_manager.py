import os
from datetime import datetime
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
import pandas as pd
import sqlite3
import tkinter as tk
import signal
import win32com.client as pythoncom
import xlwings as xw
from collections import defaultdict
import hashlib
import json

class EnhancedTranslationApplyManager:
    def __init__(self, parent_window=None):
        self.parent_ui = parent_window
        
        # ê¸°ë³¸ ìºì‹œ
        self.translation_cache = {}
        self.kr_reverse_cache = {}

        # [ìˆ˜ì •] KR ê¸°ë°˜ ë²ˆì—­ ì¶©ëŒ ë°ì´í„° ìˆ˜ì§‘ìš© ë”•ì…”ë„ˆë¦¬
        # êµ¬ì¡°: { "KRí…ìŠ¤íŠ¸": [ {"cn": "ë²ˆì—­1", "tw": "ë²ˆì—­1", "count": 2, "data": ëŒ€í‘œë°ì´í„°}, ... ] }
        self.kr_translation_conflicts = defaultdict(list)
        
        # --- [ì‹ ê·œ] ì˜êµ¬ì ì¸ ì¶©ëŒ í•´ê²° DB ê´€ë¦¬ ---
        self.db_dir = os.path.join(os.getcwd(), "user_data")
        os.makedirs(self.db_dir, exist_ok=True)
        self.resolution_db_path = os.path.join(self.db_dir, "user_resolutions.db")
        self.user_resolutions = {} # ì‚¬ìš©ìê°€ í•´ê²°í•œ ë‚´ìš©ì„ ë‹´ì„ ë”•ì…”ë„ˆë¦¬
        self._init_resolution_db() # DB íŒŒì¼ ë° í…Œì´ë¸” ì´ˆê¸°í™”
        self.load_user_resolutions() # ê¸°ì¡´ í•´ê²° ë‚´ìš© ë¡œë“œ
        # -----------------------------------------
        
        # ì§€ì› ì–¸ì–´ ì œí•œ
        self.supported_languages = ["KR", "CN", "TW"]
        
        # DB ìºì‹œ í´ë”
        self.cache_dir = os.path.join(os.getcwd(), "temp_db")
        os.makedirs(self.cache_dir, exist_ok=True)
            
        # ì„¤ì • íŒŒì¼ì—ì„œ íŠ¹ìˆ˜ ì»¬ëŸ¼ëª… ë¶ˆëŸ¬ì˜¤ê¸°
        self.SPECIAL_COLUMN_NAMES = self._load_config()

    def _load_config(self):
        """config.json íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
        config_path = 'config.json'
        default_columns = ["ë²ˆì—­ìš”ì²­", "ìˆ˜ì •ìš”ì²­", "Help", "ì›ë¬¸", "ë²ˆì—­ìš”ì²­2", "ë²ˆì—­ì¶”ê°€", "ë²ˆì—­ì ìš©"]
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get("SPECIAL_COLUMN_NAMES", default_columns)
        except FileNotFoundError:
            self.log_message(f"âš ï¸ ì„¤ì • íŒŒì¼({config_path})ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return default_columns
        except Exception as e:
            self.log_message(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return default_columns

    def _init_resolution_db(self):
        """[ì‹ ê·œ] ì¶©ëŒ í•´ê²° ë‚´ìš©ì„ ì €ì¥í•  DBë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS resolved_translations (
                    kr_text TEXT PRIMARY KEY,
                    cn_text TEXT,
                    tw_text TEXT,
                    resolved_at TEXT
                )
            ''')
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_message(f"âŒ ì‚¬ìš©ì í•´ê²° DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")

    def load_user_resolutions(self):
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("SELECT kr_text, cn_text, tw_text FROM resolved_translations")
            rows = cursor.fetchall()
            self.user_resolutions.clear() # ê¸°ì¡´ ë‚´ìš©ì„ ë¹„ìš°ê³  ìƒˆë¡œ ë¡œë“œ
            for row in rows:
                self.user_resolutions[row['kr_text']] = {'cn': row['cn_text'], 'tw': row['tw_text']}
            conn.close()
            if self.user_resolutions:
                self.log_message(f"âœ… ê¸°ì¡´ì— í•´ê²°í•œ ë²ˆì—­ ì¶©ëŒ {len(self.user_resolutions)}ê±´ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.log_message(f"âŒ ì‚¬ìš©ì í•´ê²° ë‚´ìš© ë¡œë“œ ì˜¤ë¥˜: {e}")

    def _save_resolution_to_db(self, kr_text, selected_data):
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            cursor = conn.cursor()
            resolved_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cursor.execute('''
                INSERT OR REPLACE INTO resolved_translations (kr_text, cn_text, tw_text, resolved_at)
                VALUES (?, ?, ?, ?)
            ''', (kr_text, selected_data['cn'], selected_data['tw'], resolved_at))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_message(f"âŒ í•´ê²° ë‚´ìš© DB ì €ì¥ ì˜¤ë¥˜: {e}")

    # --- [ì‹ ê·œ] í•´ê²° DB ê´€ë¦¬ìš© í•¨ìˆ˜ë“¤ ---
    def get_all_resolutions(self):
        """DBì—ì„œ ëª¨ë“  í•´ê²° ë‚´ì—­ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute("SELECT kr_text, cn_text, tw_text, resolved_at FROM resolved_translations ORDER BY resolved_at DESC")
            rows = cursor.fetchall()
            conn.close()
            return [dict(row) for row in rows]
        except Exception as e:
            self.log_message(f"âŒ í•´ê²° ë‚´ì—­ ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

    def update_resolution(self, kr_text, new_cn, new_tw):
        """DBì˜ íŠ¹ì • í•´ê²° ë‚´ì—­ì„ ìˆ˜ì •í•©ë‹ˆë‹¤."""
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            cursor = conn.cursor()
            resolved_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cursor.execute('''
                UPDATE resolved_translations 
                SET cn_text = ?, tw_text = ?, resolved_at = ?
                WHERE kr_text = ?
            ''', (new_cn, new_tw, resolved_at, kr_text))
            conn.commit()
            conn.close()
            self.load_user_resolutions() # ë©”ëª¨ë¦¬ì—ë„ ë³€ê²½ì‚¬í•­ ë°˜ì˜
            return True
        except Exception as e:
            self.log_message(f"âŒ í•´ê²° ë‚´ì—­ ìˆ˜ì • ì˜¤ë¥˜: {e}")
            return False

    def delete_resolution(self, kr_text):
        """DBì—ì„œ íŠ¹ì • í•´ê²° ë‚´ì—­ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            conn = sqlite3.connect(self.resolution_db_path)
            cursor = conn.cursor()
            cursor.execute("DELETE FROM resolved_translations WHERE kr_text = ?", (kr_text,))
            conn.commit()
            conn.close()
            self.load_user_resolutions() # ë©”ëª¨ë¦¬ì—ë„ ë³€ê²½ì‚¬í•­ ë°˜ì˜
            return True
        except Exception as e:
            self.log_message(f"âŒ í•´ê²° ë‚´ì—­ ì‚­ì œ ì˜¤ë¥˜: {e}")
            return False
                    
    def safe_strip(self, value):
        if value is None:
            return ""
        return str(value).strip()

    def safe_lower(self, value):
        if value is None:
            return ""
        return str(value).lower().strip()

    def log_message(self, message):
        if self.parent_ui and hasattr(self.parent_ui, 'log_text'):
            self.parent_ui.log_text.insert(tk.END, f"{message}\n")
            self.parent_ui.log_text.see(tk.END)
            self.parent_ui.update_idletasks()
        else:
            print(message)

    def _get_db_path(self, excel_path):
        file_hash = hashlib.md5(excel_path.encode()).hexdigest()
        return os.path.join(self.cache_dir, f"cache_{file_hash}.db")

    def _is_db_cache_valid(self, excel_path, db_path):
        if not os.path.exists(db_path):
            return False
        try:
            excel_mod_time = os.path.getmtime(excel_path)
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT value FROM metadata WHERE key = 'source_mod_time'")
            stored_mod_time = cursor.fetchone()
            conn.close()
            if stored_mod_time and float(stored_mod_time[0]) == excel_mod_time:
                self.log_message("âœ… ìœ íš¨í•œ DB ìºì‹œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
                return True
            else:
                self.log_message("âš ï¸ ì›ë³¸ íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤. DB ìºì‹œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
                return False
        except Exception as e:
            self.log_message(f"DB ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}. ìºì‹œë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
            return False

    def _build_db_from_excel(self, excel_path, db_path, progress_callback=None):
        try:
            self.log_message(f"âš™ï¸ DB ìºì‹œ êµ¬ì¶• ì‹œì‘: {os.path.basename(excel_path)}")
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute('DROP TABLE IF EXISTS metadata')
            cursor.execute('DROP TABLE IF EXISTS translation_data')
            cursor.execute('CREATE TABLE metadata (key TEXT PRIMARY KEY, value TEXT)')
            cursor.execute('''
                CREATE TABLE translation_data (
                    string_id TEXT PRIMARY KEY, kr TEXT, cn TEXT, tw TEXT,
                    file_name TEXT, sheet_name TEXT, special_columns TEXT 
                )
            ''')
            wb = load_workbook(excel_path, read_only=True, data_only=True)
            all_sheets = wb.sheetnames
            for idx, sheet_name in enumerate(all_sheets):
                if progress_callback:
                    progress_callback((idx / len(all_sheets)) * 100, f"ì‹œíŠ¸ ì²˜ë¦¬ ì¤‘ ({idx+1}/{len(all_sheets)}): {sheet_name}")
                if not sheet_name.lower().startswith("string") or sheet_name.startswith("#"):
                    continue
                ws = wb[sheet_name]
                header_map = {}
                header_row_idx = -1
                for i, row in enumerate(ws.iter_rows(min_row=1, max_row=10, values_only=True)):
                    cleaned_row = [self.safe_lower(str(cell)) for cell in row if cell is not None]
                    if 'string_id' in cleaned_row:
                        header_row_idx = i + 1
                        for col_idx, header_val in enumerate(row, 1):
                            if header_val:
                                header_map[self.safe_strip(str(header_val))] = col_idx
                        break
                if 'string_id' not in [self.safe_lower(k) for k in header_map.keys()]:
                    continue
                string_id_col_key = [k for k in header_map if self.safe_lower(k) == 'string_id'][0]
                string_id_col = header_map[string_id_col_key]
                rows_to_insert = []
                for row_data in ws.iter_rows(min_row=header_row_idx + 1, values_only=True):
                    if not row_data or len(row_data) < string_id_col or not row_data[string_id_col-1]:
                        continue
                    string_id = self.safe_strip(str(row_data[string_id_col-1]))
                    if not string_id:
                        continue
                    data = {}
                    special_data = {}
                    for header, col in header_map.items():
                        value = self.safe_strip(row_data[col-1]) if len(row_data) >= col else ""
                        normalized_header = header.lstrip('#').replace(' ', '')
                        if normalized_header in self.SPECIAL_COLUMN_NAMES:
                            special_data[normalized_header] = value
                        else:
                            data[self.safe_lower(header)] = value
                    special_columns_json = json.dumps(special_data, ensure_ascii=False)
                    kr_val = data.get("kr", "")
                    cn_val = data.get("cn", "")
                    tw_val = data.get("tw", "")
                    file_val = data.get("filename", data.get("file_name", os.path.basename(excel_path)))
                    sheet_val = data.get("sheetname", data.get("sheet_name", sheet_name))
                    rows_to_insert.append((string_id, kr_val, cn_val, tw_val, file_val, sheet_val, special_columns_json))
                cursor.executemany('INSERT OR REPLACE INTO translation_data (string_id, kr, cn, tw, file_name, sheet_name, special_columns) VALUES (?, ?, ?, ?, ?, ?, ?)', rows_to_insert)
            excel_mod_time = os.path.getmtime(excel_path)
            cursor.execute("INSERT INTO metadata (key, value) VALUES (?, ?)", ('source_mod_time', str(excel_mod_time)))
            conn.commit()
            conn.close()
            self.log_message("âœ… DB ìºì‹œ êµ¬ì¶• ì™„ë£Œ.")
            return {"status": "success"}
        except Exception as e:
            self.log_message(f"âŒ DB ìºì‹œ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    def initiate_excel_caching(self, excel_path, force_rebuild=False, progress_callback=None):
        db_path = self._get_db_path(excel_path)
        if force_rebuild:
            self.log_message("â„¹ï¸ ì‚¬ìš©ìì˜ ìš”ì²­ìœ¼ë¡œ DB ìºì‹œë¥¼ ê°•ì œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.")
        elif self._is_db_cache_valid(excel_path, db_path):
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT DISTINCT sheet_name FROM translation_data ORDER BY sheet_name")
                sheets = [row[0] for row in cursor.fetchall()]
                conn.close()
                return {"status": "success", "source_type": "DB_Cache", "sheets": sheets}
            except Exception as e:
                self.log_message(f"ìºì‹œëœ ì‹œíŠ¸ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
        build_result = self._build_db_from_excel(excel_path, db_path, progress_callback)
        if build_result["status"] == "success":
            try:
                conn = sqlite3.connect(db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT DISTINCT sheet_name FROM translation_data ORDER BY sheet_name")
                sheets = [row[0] for row in cursor.fetchall()]
                conn.close()
                return {"status": "success", "source_type": "DB_Cache", "sheets": sheets}
            except Exception as e:
                self.log_message(f"ì¬êµ¬ì¶• í›„ ì‹œíŠ¸ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {e}")
                return {"status": "error", "message": "DB ì¬êµ¬ì¶• í›„ ì‹œíŠ¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}
        else:
            return build_result
        
    def load_translation_cache_from_excel_with_filter(self, excel_path, sheet_names, special_column_filter=None):
        self.log_message("âš™ï¸ DB ìºì‹œë¡œë¶€í„° ë©”ëª¨ë¦¬ ìºì‹œ ë¡œë”© ì‹œì‘...")
        db_path = self._get_db_path(excel_path)
        if not os.path.exists(db_path):
            return {"status": "error", "message": "DB ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

        try:
            self.translation_cache = {}
            self.kr_reverse_cache = {}
            self.kr_translation_conflicts = defaultdict(list)
            
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            query = "SELECT * FROM translation_data"
            params = []
            if sheet_names:
                query += f" WHERE sheet_name IN ({','.join('?' for _ in sheet_names)})"
                params.extend(sheet_names)
            cursor.execute(query, params)
            rows = cursor.fetchall()
            conn.close()

            temp_translation_cache = {}
            normalized_filter_column = ""
            if special_column_filter:
                raw_filter_column = special_column_filter.get('column_name', '')
                normalized_filter_column = raw_filter_column.lstrip('#').replace(' ', '')
                filter_value = special_column_filter.get('condition_value', '')

            for row in rows:
                data = dict(row)
                if special_column_filter:
                    special_columns = json.loads(data.get('special_columns', '{}'))
                    if normalized_filter_column in special_columns:
                        special_cell_value = special_columns[normalized_filter_column]
                        if self.safe_lower(filter_value) in self.safe_lower(special_cell_value):
                            temp_translation_cache[data["string_id"]] = data
                    else:
                        continue
                else:
                    temp_translation_cache[data["string_id"]] = data
            self.translation_cache = temp_translation_cache
            
            kr_candidates = defaultdict(lambda: defaultdict(list))
            for string_id, data in self.translation_cache.items():
                kr_text = self.safe_strip(data.get("kr", ""))
                if not kr_text: continue
                
                # [ìˆ˜ì •] ì‚¬ìš©ìê°€ ì´ë¯¸ í•´ê²°í•œ ë‚´ìš©ì´ ìˆë‹¤ë©´, ê·¸ê²ƒìœ¼ë¡œ ë°ì´í„°ë¥¼ í†µì¼
                if kr_text in self.user_resolutions:
                    cn_text = self.user_resolutions[kr_text]['cn']
                    tw_text = self.user_resolutions[kr_text]['tw']
                else:
                    cn_text = self.safe_strip(data.get("cn", ""))
                    tw_text = self.safe_strip(data.get("tw", ""))

                translation_pair = (cn_text, tw_text)
                kr_candidates[kr_text][translation_pair].append(data)

            for kr_text, pairs in kr_candidates.items():
                if len(pairs) == 1:
                    self.kr_reverse_cache[kr_text] = list(pairs.values())[0][0]
                else:
                     for (cn, tw), data_list in pairs.items():
                        self.kr_translation_conflicts[kr_text].append({
                            "cn": cn, "tw": tw, "count": len(data_list), "data": data_list[0]
                        })

            self.log_message(f"ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œ êµ¬ì„± ì™„ë£Œ:")
            self.log_message(f" Â - ìµœì¢… ë¡œë“œëœ STRING_ID: {len(self.translation_cache)}ê°œ")
            if special_column_filter:
                self.log_message(f" Â - í•„í„° ì¡°ê±´ ì ìš©: '{normalized_filter_column}' = '{filter_value}'")
            if self.kr_translation_conflicts:
                self.log_message(f" Â - âš ï¸ KR ê¸°ë°˜ ë²ˆì—­ ì¶©ëŒ ê°ì§€: {len(self.kr_translation_conflicts)}ê°œ í…ìŠ¤íŠ¸")

            return {
                "status": "success", "source_type": "Excel_DB_Cache",
                "id_count": len(self.translation_cache),
                "filtered_count": len(self.translation_cache) if special_column_filter else 0,
                "conflict_count": len(self.kr_translation_conflicts)
            }
        except Exception as e:
            self.log_message(f"âŒ DB ìºì‹œ ë¡œë”© ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}

    def get_translation_conflicts(self):
        return self.kr_translation_conflicts
        
    def update_resolved_translations(self, resolutions):
        """[ìˆ˜ì •] í•´ê²°ëœ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì™€ ì˜êµ¬ DBì— ëª¨ë‘ ì €ì¥"""
        resolved_count = 0
        for kr_text, selected_data in resolutions.items():
            # 1. ë©”ëª¨ë¦¬ ìºì‹œ ì—…ë°ì´íŠ¸ (í˜„ì¬ ì„¸ì…˜ìš©)
            if kr_text not in self.kr_reverse_cache:
                self.kr_reverse_cache[kr_text] = selected_data
                resolved_count += 1
            
            # 2. ì˜êµ¬ DBì— ì €ì¥ (ë‹¤ìŒ ì„¸ì…˜ìš©)
            self._save_resolution_to_db(kr_text, selected_data)

        self.log_message(f"âœ… ë²ˆì—­ ì¶©ëŒ í•´ê²°: {resolved_count}ê°œ í•­ëª©ì´ ì—…ë°ì´íŠ¸ ë° ì˜êµ¬ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        for kr_text in resolutions:
            if kr_text in self.kr_translation_conflicts:
                del self.kr_translation_conflicts[kr_text]

    def apply_translation_with_filter_option(self, file_path, options):
        mode = options.get("mode", "id")
        selected_langs = options.get("selected_langs", [])
        record_date = options.get("record_date", True)
        use_filtered_data = options.get("use_filtered_data", False)

        kr_match_check = options.get("kr_match_check", True)
        kr_mismatch_delete = options.get("kr_mismatch_delete", False)
        kr_overwrite = options.get("kr_overwrite", False)
        kr_overwrite_on_kr_mode = options.get("kr_overwrite_on_kr_mode", False)
        allowed_statuses = options.get("allowed_statuses", [])
        allowed_statuses_lower = [status.lower() for status in allowed_statuses] if allowed_statuses else []

        active_cache = self.translation_cache
        cache_type = "íŠ¹ìˆ˜í•„í„°ë§" if use_filtered_data else "ì „ì²´"

        if not active_cache:
            return {"status": "error", "message": f"{cache_type} ë²ˆì—­ ìºì‹œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        if mode == 'kr' and not self.kr_reverse_cache:
            return {"status": "error", "message": "KR ê¸°ë°˜ ì ìš©ì„ ìœ„í•œ ì—­ë°©í–¥ ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤."}

        file_name = os.path.basename(file_path)

        option_summary = [f"{mode.upper()} ê¸°ë°˜", f"{cache_type} ìºì‹œ"]
        if mode == 'id' and kr_match_check:
            option_summary.append("KRì¼ì¹˜ê²€ì‚¬")
            if kr_mismatch_delete: option_summary.append("ë¶ˆì¼ì¹˜ì‹œì‚­ì œ")
            if kr_overwrite: option_summary.append("ë®ì–´ì“°ê¸°")
        elif mode == 'kr' and kr_overwrite_on_kr_mode:
            option_summary.append("ë®ì–´ì“°ê¸°")
        if allowed_statuses: option_summary.append(f"ì¡°ê±´:{','.join(allowed_statuses)}")
        self.log_message(f"ğŸ“ {file_name} ì²˜ë¦¬ì‹œì‘ [{' | '.join(option_summary)}]")

        workbook = None
        try:
            workbook = load_workbook(file_path)
            string_sheets = [sheet for sheet in workbook.sheetnames if sheet.lower().startswith("string") and not sheet.startswith("#")]
            if not string_sheets:
                self.log_message(f" Â  âš ï¸ String ì‹œíŠ¸ ì—†ìŒ")
                return {"status": "info", "message": "íŒŒì¼ì— String ì‹œíŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤"}

            file_modified = False
            results = defaultdict(int)
            overwritten_items = []

            fill_green = PatternFill(start_color="DAF2D0", end_color="DAF2D0", fill_type="solid")
            fill_orange = PatternFill(start_color="FFDDC1", end_color="FFDDC1", fill_type="solid")
            fill_blue = PatternFill(start_color="D0E7FF", end_color="D0E7FF", fill_type="solid")

            for sheet_name in string_sheets:
                worksheet = workbook[sheet_name]
                string_id_col, header_row = self.find_string_id_position(worksheet)
                if not string_id_col or not header_row:
                    self.log_message(f" Â  âš ï¸ {sheet_name}: STRING_ID ì»¬ëŸ¼ ì—†ìŒ")
                    continue

                supported_langs = [lang for lang in selected_langs if lang in self.supported_languages]
                lang_cols = self.find_language_columns(worksheet, header_row, supported_langs + ['KR'])
                request_col_idx = self.find_target_columns(worksheet, header_row, ["#ë²ˆì—­ìš”ì²­"]).get("#ë²ˆì—­ìš”ì²­")
                sheet_stats = defaultdict(int)
                lang_apply_count = {lang: 0 for lang in supported_langs if lang != 'KR'}
                
                for row_idx in range(header_row + 1, worksheet.max_row + 1):
                    if allowed_statuses_lower and request_col_idx:
                        request_val = self.safe_lower(str(worksheet.cell(row=row_idx, column=request_col_idx).value or ''))
                        if request_val not in allowed_statuses_lower:
                            sheet_stats["conditional_skipped"] += 1
                            continue

                    trans_data = None
                    key_value = ''
                    if mode == 'id':
                        key_value = self.safe_strip(str(worksheet.cell(row=row_idx, column=string_id_col).value or ''))
                        if key_value: trans_data = active_cache.get(key_value)
                    else: 
                        if 'KR' in lang_cols:
                            key_value = self.safe_strip(str(worksheet.cell(row=row_idx, column=lang_cols['KR']).value or ''))
                            if key_value: trans_data = self.kr_reverse_cache.get(key_value)

                    if not key_value or not trans_data: continue

                    row_modified_this_iteration = False
                    if mode == 'id' and kr_match_check:
                        current_kr_val = self.safe_strip(str(worksheet.cell(row=row_idx, column=lang_cols['KR']).value or ''))
                        cache_kr_val = self.safe_strip(str(trans_data.get('kr', '')))
                        if current_kr_val != cache_kr_val:
                            if kr_mismatch_delete:
                                deleted_count = 0
                                for lang, col_idx in lang_cols.items():
                                    if lang != 'KR' and worksheet.cell(row=row_idx, column=col_idx).value:
                                        worksheet.cell(row=row_idx, column=col_idx).value = ""
                                        deleted_count += 1
                                        row_modified_this_iteration = True
                                if deleted_count > 0: sheet_stats["kr_mismatch_deleted"] += 1
                            else:
                                sheet_stats["kr_mismatch_skipped"] += 1
                            continue 

                    for lang in supported_langs:
                        if lang == 'KR': continue
                        lang_lower = lang.lower()
                        col_idx = lang_cols.get(lang)
                        if not col_idx: continue
                        cell = worksheet.cell(row=row_idx, column=col_idx)
                        current_val = self.safe_strip(str(cell.value or ''))
                        cached_val = self.safe_strip(str(trans_data.get(lang_lower, '')))
                        if cached_val and current_val != cached_val:
                            should_overwrite = (mode == 'id' and kr_match_check and kr_overwrite) or \
                                            (mode == 'kr' and kr_overwrite_on_kr_mode)
                            if should_overwrite:
                                original_text = cell.value
                                cell.value = cached_val
                                cell.fill = fill_orange
                                sheet_stats["overwritten"] += 1
                                lang_apply_count[lang] += 1
                                row_modified_this_iteration = True
                                overwritten_items.append({
                                    "file_name": file_name, "sheet_name": sheet_name,
                                    "string_id": trans_data.get('string_id', key_value),
                                    "language": lang, "kr_text": trans_data.get('kr', ''),
                                    "original_text": self.safe_strip(original_text),
                                    "overwritten_text": cached_val
                                })
                            elif not current_val:
                                cell.value = cached_val
                                cell.fill = fill_blue if use_filtered_data else fill_green
                                sheet_stats["updated"] += 1
                                lang_apply_count[lang] += 1
                                row_modified_this_iteration = True
                    if row_modified_this_iteration:
                        file_modified = True
                        if record_date and request_col_idx:
                            worksheet.cell(row=row_idx, column=request_col_idx).value = "íŠ¹ìˆ˜í•„í„°ì ìš©" if use_filtered_data else "ì ìš©"

                if sheet_stats["updated"] > 0 or sheet_stats["overwritten"] > 0:
                    lang_details = [f"{lang}:{count}" for lang, count in lang_apply_count.items() if count > 0]
                    log_parts = []
                    if sheet_stats["updated"] > 0: log_parts.append(f"ì‹ ê·œ:{sheet_stats['updated']}")
                    if sheet_stats["overwritten"] > 0: log_parts.append(f"ë®ì–´ì”€:{sheet_stats['overwritten']}")
                    if lang_details: log_parts.append(f"[{', '.join(lang_details)}]")
                    self.log_message(f" Â  âœ… {sheet_name}: {' | '.join(log_parts)}")
                else:
                    skip_reasons = []
                    if sheet_stats["conditional_skipped"] > 0: skip_reasons.append(f"ì¡°ê±´ë¶ˆì¼ì¹˜:{sheet_stats['conditional_skipped']}")
                    if sheet_stats["kr_mismatch_skipped"] > 0: skip_reasons.append(f"KRë¶ˆì¼ì¹˜:{sheet_stats['kr_mismatch_skipped']}")
                    if sheet_stats["kr_mismatch_deleted"] > 0: skip_reasons.append(f"KRë¶ˆì¼ì¹˜ì‚­ì œ:{sheet_stats['kr_mismatch_deleted']}")
                    if skip_reasons: self.log_message(f" Â  âš ï¸ {sheet_name}: ì ìš©ì—†ìŒ ({' | '.join(skip_reasons)})")
                    else: self.log_message(f" Â  âš ï¸ {sheet_name}: ì ìš©ì—†ìŒ (ë²ˆì—­ë°ì´í„° ì—†ìŒ)")

                for key, val in sheet_stats.items():
                    results[f"total_{key}"] += val

            if file_modified:
                self.log_message(f" Â  ğŸ’¾ ë³€ê²½ì‚¬í•­ ì €ì¥ ì¤‘...")
                workbook.save(file_path)
                summary_parts = []
                if results["total_updated"] > 0: summary_parts.append(f"ì‹ ê·œ {results['total_updated']}ê°œ")
                if results["total_overwritten"] > 0: summary_parts.append(f"ë®ì–´ì”€ {results['total_overwritten']}ê°œ")
                total_applied = results["total_updated"] + results["total_overwritten"]
                cache_info = f"({cache_type}ìºì‹œ)" if use_filtered_data else ""
                self.log_message(f" Â  âœ… {file_name} ì™„ë£Œ: {' | '.join(summary_parts)} (ì´ {total_applied}ê°œ ì ìš©) {cache_info}")
            else:
                skip_summary = []
                if results["total_conditional_skipped"] > 0: skip_summary.append(f"ì¡°ê±´ {results['total_conditional_skipped']}ê°œ")
                if results["total_kr_mismatch_skipped"] > 0: skip_summary.append(f"KRë¶ˆì¼ì¹˜ {results['total_kr_mismatch_skipped']}ê°œ")
                if skip_summary: self.log_message(f" Â  âš ï¸ {file_name} ì™„ë£Œ: ë³€ê²½ì—†ìŒ ({' | '.join(skip_summary)} ê±´ë„ˆëœ€)")
                else: self.log_message(f" Â  âš ï¸ {file_name} ì™„ë£Œ: ë³€ê²½ì—†ìŒ (ë²ˆì—­ ë°ì´í„° ì—†ìŒ)")

            final_results = {key: val for key, val in results.items()}
            return {"status": "success", **final_results, "overwritten_items": overwritten_items}
        except Exception as e:
            self.log_message(f" Â  âŒ {file_name} ì˜¤ë¥˜: {str(e)}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e), "error_type": "processing_error"}
        finally:
            if workbook:
                workbook.close()
 
    def find_string_id_position(self, worksheet):
        for row in range(2, 6):
            for col in range(1, min(10, worksheet.max_column + 1)):
                cell_value = worksheet.cell(row=row, column=col).value
                if cell_value and isinstance(cell_value, str):
                    if "string_id" in self.safe_lower(cell_value):
                        return col, row
        for row in worksheet.iter_rows(min_row=1, max_row=1, max_col=5):
            for cell in row:
                if cell.value and isinstance(cell.value, str):
                    if "string_id" in self.safe_lower(cell.value):
                        return cell.column, cell.row
        return None, None

    def find_language_columns(self, worksheet, header_row, langs):
        if not header_row: return {}
        lang_cols = {}
        langs_upper = [lang.upper() for lang in langs]
        for row in worksheet.iter_rows(min_row=header_row, max_row=header_row):
            for cell in row:
                if not cell.value: continue
                header_text = self.safe_strip(cell.value).upper()
                if header_text in langs_upper:
                    lang_cols[header_text] = cell.column
        return lang_cols

    def find_target_columns(self, worksheet, header_row, target_columns=None):
        if not header_row: return {}
        found_columns = {}
        all_targets = ["#ë²ˆì—­ìš”ì²­"]
        if target_columns: all_targets.extend(target_columns)
        all_targets = list(set(all_targets))
        for cell in worksheet[header_row]:
            if cell.value and isinstance(cell.value, str):
                cell_value_clean = self.safe_strip(cell.value).lower()
                for target in all_targets:
                    if cell_value_clean == target.lower():
                        found_columns[target] = cell.column
                        break
        return found_columns

    def detect_special_column_in_excel(self, excel_path, target_column_name):
        db_path = self._get_db_path(excel_path)
        if not os.path.exists(db_path):
            return {"status": "error", "message": "DB ìºì‹œê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}
        self.log_message(f"âš™ï¸ [íŠ¹ìˆ˜ì»¬ëŸ¼ê°ì§€] DB ìºì‹œì—ì„œ '{target_column_name}' ë¶„ì„ ì‹œì‘...")
        normalized_target_column = target_column_name.lstrip('#').replace(' ', '')
        self.log_message(f"   (ì •ê·œí™”ëœ ê²€ìƒ‰ì–´: '{normalized_target_column}')")
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT special_columns, sheet_name FROM translation_data")
            rows = cursor.fetchall()
            conn.close()
            values_count = defaultdict(int)
            found_in_sheets = set()
            non_empty_rows = 0
            for special_json, sheet_name in rows:
                if not special_json: continue
                special_data = json.loads(special_json)
                if normalized_target_column in special_data:
                    value = self.safe_strip(special_data[normalized_target_column])
                    if value:
                        values_count[value] += 1
                        found_in_sheets.add(sheet_name)
                        non_empty_rows += 1
            if not values_count:
                self.log_message(f"âš ï¸ íŠ¹ìˆ˜ ì»¬ëŸ¼ '{target_column_name}'ì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "success", "detected_info": {}}
            most_common = sorted(values_count.items(), key=lambda x: x[1], reverse=True)[:5]
            analysis_result = {
                'non_empty_rows': non_empty_rows, 'unique_values': dict(values_count),
                'most_common': most_common, 'found_in_sheets': list(found_in_sheets)
            }
            self.log_message(f"âœ… íŠ¹ìˆ˜ ì»¬ëŸ¼ '{target_column_name}' ë¶„ì„ ì™„ë£Œ:")
            self.log_message(f" Â - ë°œê²¬ëœ ì‹œíŠ¸: {len(found_in_sheets)}ê°œ")
            self.log_message(f" Â - ë°ì´í„° í•­ëª©: {non_empty_rows}ê°œ / ê³ ìœ ê°’: {len(values_count)}ê°œ")
            self.log_message(f" Â - ìµœë¹ˆê°’: {most_common}")
            return {"status": "success", "detected_info": analysis_result}
        except Exception as e:
            self.log_message(f"âŒ íŠ¹ìˆ˜ ì»¬ëŸ¼ ë¶„ì„ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return {"status": "error", "message": str(e)}